{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64497566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch \n",
    "import numpy as np \n",
    "import shutil \n",
    "import json \n",
    "import zipfile \n",
    "import io \n",
    "import pytorch_lightning as pl \n",
    "from scipy.sparse import csc_matrix \n",
    "from pathlib import Path \n",
    "from pytorch_tabnet.utils import (\n",
    "    create_explain_matrix,\n",
    "    ComplexEncoder,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy, precision, recall \n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "import sys, os \n",
    "sys.path.append('../src')\n",
    "\n",
    "from torchmetrics.functional import *\n",
    "import torchmetrics \n",
    "\n",
    "from data import *\n",
    "from lightning_train import *\n",
    "from model import *\n",
    "\n",
    "from torchmetrics.functional.classification.stat_scores import _stat_scores_update, _stat_scores\n",
    "from sklearn.metrics import classification_report\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c0990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network\n",
      "Initializing explain matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/val/test DataLoaders...\n",
      "Done, continuing to training.\n",
      "Calculating weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8997e2db881d4897858f86ec83f25e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median f1 score is 0.9615384615384616 for epoch=0\n",
      "Test F1 scores are [0.96153846 0.         0.97967318 0.97609329 0.5794702  0.\n",
      " 0.         0.96940024 0.98089172]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.9392621517181396,\n",
      " 'test_precision': 0.9392621517181396,\n",
      " 'test_recall': 0.9392621517181396}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/Documents/Projects/SIMS/notebooks/../src/model.py:177: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_accuracy': 0.9392621517181396,\n",
       "  'test_precision': 0.9392621517181396,\n",
       "  'test_recall': 0.9392621517181396}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = DataModule(\n",
    "    datafiles=['../data/dental/human_dental_T.h5ad'],\n",
    "    labelfiles=['../data/dental/labels_human_dental.tsv'],\n",
    "    class_label='cell_type',\n",
    "    sep='\\t',\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "#     subset=list(range(1000)),\n",
    "    stratify=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "trained = TabNetLightning.load_from_checkpoint(\n",
    "    '../checkpoints/checkpoint-80-desc-dental.ckpt',\n",
    "    input_dim=module.num_features,\n",
    "    output_dim=module.num_labels,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.test(trained, datamodule=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe84492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "# def confusion_matrix(dataloader):\n",
    "#     confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "#     with torch.no_grad():\n",
    "#         for i, (inputs, classes) in enumerate(tqdm(dataloader)):\n",
    "#             outputs, _ = model(inputs)\n",
    "            \n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "#                     confusion_matrix[t.long(), p.long()] += 1\n",
    "                    \n",
    "#     return confusion_matrix \n",
    "\n",
    "# def median_f1(tps, fps, fns):\n",
    "#     precisions = tps / (tps+fps)\n",
    "#     recalls = tps / (tps+fns)\n",
    "    \n",
    "#     f1s = 2*(np.dot(precisions, recalls)) / (precisions + recalls)\n",
    "    \n",
    "#     return np.nanmedian(f1s)\n",
    "\n",
    "# def per_class_f1(*args, **kwargs):\n",
    "#     res = torchmetrics.functional.f1_score(*args, **kwargs, average='none')\n",
    "#     return res\n",
    "\n",
    "# def per_class_precision(*args, **kwargs):\n",
    "#     res = torchmetrics.functional.precision(*args, **kwargs, average='none')\n",
    "    \n",
    "#     return res\n",
    "\n",
    "# def per_class_recall(*args, **kwargs):\n",
    "#     res = torchmetrics.functional.precision(*args, **kwargs, average='none')\n",
    "    \n",
    "#     return res \n",
    "\n",
    "# def weighted_accuracy(*args, **kwargs):\n",
    "#     res = torchmetrics.functional.accuracy(*args, **kwargs, average='weighted')\n",
    "    \n",
    "#     return res \n",
    "\n",
    "# def balanced_accuracy(*args, **kwargs):\n",
    "#     res = torchmetrics.functional.accuracy(*args, **kwargs, average='macro')\n",
    "    \n",
    "#     return res \n",
    "\n",
    "def aggregate_metrics(num_classes) -> Dict[str, Callable]:\n",
    "    metrics = {\n",
    "        # Accuracies\n",
    "        'total_accuracy': torchmetrics.functional.accuracy,\n",
    "        'balanced_accuracy': partial(balanced_accuracy, num_classes=num_classes),\n",
    "        'weighted_accuracy': weighted_accuracy,\n",
    "        \n",
    "        # Precision, recall and f1s\n",
    "        'precision': torchmetrics.functional.precision,\n",
    "        'recall': torchmetrics.functional.recall,\n",
    "        'f1': torchmetrics.functional.f1_score,\n",
    "        \n",
    "        # Per class \n",
    "        'per_class_f1': per_class_f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall,\n",
    "    }\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c46c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetLightning(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        cat_idxs=[],\n",
    "        cat_dims=[],\n",
    "        cat_emb_dim=1,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        epsilon=1e-15,\n",
    "        virtual_batch_size=128,\n",
    "        momentum=0.02,\n",
    "        mask_type=\"sparsemax\",\n",
    "        lambda_sparse = 1e-3,\n",
    "        optim_params: Dict[str, float]={\n",
    "            'optimizer': torch.optim.Adam,\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 0.01,\n",
    "        },\n",
    "        metrics: Dict[str, Callable]=None,\n",
    "        scheduler_params: Dict[str, float]=None,\n",
    "        weights=None,\n",
    "        loss=None, # will default to cross_entropy\n",
    "        pretrained=None,\n",
    "        no_explain=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Stuff needed for training\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "\n",
    "        self.optim_params = optim_params\n",
    "        self.scheduler_params = scheduler_params\n",
    "        \n",
    "        self.weights = weights \n",
    "        self.loss = loss \n",
    "\n",
    "        if pretrained is not None:\n",
    "            self._from_pretrained(**pretrained.get_params())\n",
    "        # self.device = ('cuda:0' if torch.cuda.is_available() else 'cpu!')\n",
    "        \n",
    "        if metrics is None:\n",
    "            self.metrics = aggregate_metrics()\n",
    "        else:\n",
    "            self.metrics = metrics \n",
    "            \n",
    "        print(f'Initializing network')\n",
    "        self.network = TabNet(\n",
    "            input_dim=input_dim, \n",
    "            output_dim=output_dim, \n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            mask_type=mask_type,\n",
    "        )\n",
    "        \n",
    "        if not no_explain:\n",
    "            print(f'Initializing explain matrix')\n",
    "            self.reducing_matrix = create_explain_matrix(\n",
    "                self.network.input_dim,\n",
    "                self.network.cat_emb_dim,\n",
    "                self.network.cat_idxs,\n",
    "                self.network.post_embed_dim,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def _compute_loss(self, y, y_hat):\n",
    "        # If user doesn't specify, just set to cross_entropy\n",
    "        if self.loss is None:\n",
    "            self.loss = F.cross_entropy \n",
    "\n",
    "        return self.loss(y, y_hat, weight=self.weights)\n",
    "\n",
    "    def _step(self, batch, tag):\n",
    "        x, y = batch\n",
    "        y_hat, M_loss = self.network(x)\n",
    "\n",
    "        loss = self._compute_loss(y_hat, y)\n",
    "        # Add the overall sparsity loss\n",
    "        loss = loss - self.lambda_sparse * M_loss\n",
    "        self._compute_metrics(y_hat, y, tag)\n",
    "        \n",
    "        tp, fp, _, fn = _stat_scores_update(\n",
    "            preds=y_hat,\n",
    "            target=y,\n",
    "            num_classes=self.output_dim,\n",
    "            reduce=\"macro\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "        }\n",
    "\n",
    "    # Calculations on step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'test')\n",
    "    \n",
    "    def _epoch_end(self, step_outputs):\n",
    "        tps, fps, fns = [], [], []\n",
    "        \n",
    "        for i in range(len(step_outputs)):\n",
    "            res = step_outputs[i]\n",
    "            tp, fp, fn = res['tp'], res['fp'], res['fn']\n",
    "                \n",
    "            tps.append(tp.numpy())\n",
    "            fps.append(fp.numpy())\n",
    "            fns.append(fn.numpy())\n",
    "            \n",
    "        tp = np.sum(np.array(tps), axis=0)\n",
    "        fp = np.sum(np.array(fps), axis=0)\n",
    "        fn = np.sum(np.array(fns), axis=0)\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1s = 2*(precision * recall) / (precision + recall)\n",
    "        f1s = np.nan_to_num(f1s)\n",
    "        print(f\"Median f1 score is {np.nanmedian(f1s)} for epoch={self.current_epoch}\")\n",
    "\n",
    "    # Calculation on epoch end, for \"median F1 score\"\n",
    "    def training_epoch_end(self, step_outputs):\n",
    "        self._epoch_end(step_outputs)\n",
    "        \n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        self._epoch_end(step_outputs) \n",
    "    \n",
    "    def test_epoch_end(self, step_outputs):\n",
    "        self._epoch_end(step_outputs) \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if 'optimizer' in self.optim_params:\n",
    "            optimizer = self.optim_params.pop('optimizer')\n",
    "            optimizer = optimizer(self.parameters(), **self.optim_params)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=0.2, weight_decay=1e-5)\n",
    "\n",
    "        if self.scheduler_params is not None:\n",
    "            scheduler = self.scheduler_params.pop('scheduler')\n",
    "            scheduler = scheduler(optimizer, **self.scheduler_params)\n",
    "\n",
    "        if self.scheduler_params is None:\n",
    "            return optimizer\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss',\n",
    "        }\n",
    "    \n",
    "    def _compute_metrics(self, \n",
    "        y_hat: torch.Tensor, \n",
    "        y: torch.Tensor, \n",
    "        tag: str,\n",
    "        on_epoch=True, \n",
    "        on_step=False,\n",
    "    ):\n",
    "        metrics = {}\n",
    "        for name, metric in self.metrics.items():\n",
    "            val = metric(y_hat, y)\n",
    "            metrics[name] = val\n",
    "            self.log(\n",
    "                f\"{tag}_{name}\", \n",
    "                val, \n",
    "                on_epoch=on_epoch, \n",
    "                on_step=on_step,\n",
    "                logger=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d961b0",
   "metadata": {},
   "source": [
    "# Test Module with Small Dental Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d3b0a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/val/test DataLoaders...\n",
      "Done, continuing to training.\n",
      "Calculating weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network\n"
     ]
    }
   ],
   "source": [
    "module = DataModule(\n",
    "    datafiles=['../data/dental/human_dental_T.h5ad'],\n",
    "    labelfiles=['../data/dental/labels_human_dental.tsv'],\n",
    "    class_label='cell_type',\n",
    "    sep='\\t',\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "#     subset=list(range(1000)),\n",
    "    stratify=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "module.setup()\n",
    "wandb_logger = WandbLogger(\n",
    "    project=f\"custom metric tests\",\n",
    "    name='Dental Model, First 500 samples'\n",
    ")\n",
    "\n",
    "model = TabNetLightning(\n",
    "    input_dim=module.num_features,\n",
    "    output_dim=module.num_labels,\n",
    "    no_explain=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "#   logger=wandb_logger,\n",
    ")\n",
    "\n",
    "# trainer.fit(model, datamodule=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a0c1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network\n",
      "Initializing explain matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406a53b33da7402eb95bfd4d11eafa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "When you set `average` as macro, you have to provide the number of classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trained \u001b[38;5;241m=\u001b[39m TabNetLightning\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../checkpoints/checkpoint-80-desc-dental.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mnum_features,\n\u001b[1;32m      4\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mnum_labels,\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:906\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, test_dataloaders)\u001b[0m\n\u001b[1;32m    901\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.test(dataloaders)` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    905\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m test_dataloaders\n\u001b[0;32m--> 906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:949\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtested_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    945\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    946\u001b[0m )\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# run test\u001b[39;00m\n\u001b[0;32m--> 949\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtested_ckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1195\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1270\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1270\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_evaluating\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:206\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_evaluating\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_evaluating\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the test loop\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1281\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__setup_profiler()\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1329\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1329\u001b[0m     eval_loop_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;66;03m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:109\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fetcher \u001b[38;5;241m=\u001b[39m dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    105\u001b[0m     dataloader, dataloader_idx\u001b[38;5;241m=\u001b[39mdataloader_idx\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m dl_max_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 109\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dataloaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_step_and_end\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 122\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:213\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:244\u001b[0m, in \u001b[0;36mAccelerator.test_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"The actual test step.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.test_step` for more details\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtest_step_context():\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:222\u001b[0m, in \u001b[0;36mTrainingTypePlugin.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTabNetLightning.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTabNetLightning._step\u001b[0;34m(self, batch, tag)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n\u001b[1;32m     98\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_sparse \u001b[38;5;241m*\u001b[39m M_loss\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m tp, fp, _, fn \u001b[38;5;241m=\u001b[39m _stat_scores_update(\n\u001b[1;32m    102\u001b[0m     preds\u001b[38;5;241m=\u001b[39my_hat,\n\u001b[1;32m    103\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    104\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim,\n\u001b[1;32m    105\u001b[0m     reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m\"\u001b[39m: tp,\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m\"\u001b[39m: fp,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m\"\u001b[39m: fn,\n\u001b[1;32m    113\u001b[0m }\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mTabNetLightning._compute_metrics\u001b[0;34m(self, y_hat, y, tag, on_epoch, on_step)\u001b[0m\n\u001b[1;32m    183\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 185\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     metrics[name] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    189\u001b[0m         val, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/SIMS/notebooks/../src/metrics.py:55\u001b[0m, in \u001b[0;36mbalanced_accuracy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalanced_accuracy\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtorchmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torchmetrics/functional/classification/accuracy.py:396\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(preds, target, average, mdmc_average, threshold, top_k, subset_accuracy, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `average` has to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_average\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m num_classes \u001b[38;5;129;01mor\u001b[39;00m num_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen you set `average` as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you have to provide the number of classes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    398\u001b[0m allowed_mdmc_average \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamplewise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mdmc_average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_mdmc_average:\n",
      "\u001b[0;31mValueError\u001b[0m: When you set `average` as macro, you have to provide the number of classes."
     ]
    }
   ],
   "source": [
    "trained = TabNetLightning.load_from_checkpoint(\n",
    "    '../checkpoints/checkpoint-80-desc-dental.ckpt',\n",
    "    input_dim=module.num_features,\n",
    "    output_dim=module.num_labels,\n",
    ")\n",
    "\n",
    "trainer.test(trained, datamodule=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for X, y in module.trainloader:\n",
    "    labels.extend(list(y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for X, y in module.trainloader:\n",
    "    print(y.numpy())\n",
    "    labels.extend(list(y.numpy()))\n",
    "    \n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51804c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for X, y in module.valloader:\n",
    "    labels.extend(list(y.numpy()))\n",
    "    \n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for X, y in module.testloader:\n",
    "    labels.extend(list(y.numpy()))\n",
    "    \n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../data/dental/labels_human_dental.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd98b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = DataModule(\n",
    "    datafiles=['../data/dental/human_dental_T.h5ad'],\n",
    "    labelfiles=['../data/dental/labels_human_dental.tsv'],\n",
    "    class_label='cell_type',\n",
    "    sep='\\t',\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    subset=list(range(1000)),\n",
    ")\n",
    "\n",
    "module.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1134897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science] *",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
